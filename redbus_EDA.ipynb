{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9b8777-4694-4f13-a9c9-a234f2482164",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "## 1. [Libraries](#1.-Import-the-Libraries)\n",
    "## 2. [Analysis](#2.-Preliminary-Analysis)\n",
    "## [Preprocessing]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc1e14-ecca-4ddd-a39c-53f1e6d6e347",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸšŒ Bus Demand Forecasting Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349be54c-3f9c-4a6e-89a6-95974d010e29",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Problem Statement\n",
    "\n",
    "Accurately forecasting demand for bus journeys is a complex task due to the influence of multiple factors. These include:\n",
    "\n",
    "- **Holiday calendars**\n",
    "- **Wedding seasons**\n",
    "- **Long weekends**\n",
    "- **School vacations**\n",
    "- **Exam schedules**\n",
    "- **Day-of-week effects**\n",
    "- **Regional holidays** (impact varies by region)\n",
    "\n",
    "> Not all holidays cause significant changes in demand, making this a non-trivial modeling problem.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Objective\n",
    "\n",
    "Develop a predictive model that forecasts the **total number of seats booked** for a given **route** and **date of journey**, exactly **15 days before the travel date**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‚ Provided Data\n",
    "\n",
    "You will be given historical data from the platform, which includes:\n",
    "\n",
    "- `seats_booked`: Number of seats booked (actual demand)\n",
    "- `date_of_journey`: The actual travel date\n",
    "- `date_of_issue`: The date when the ticket was booked\n",
    "- `search_data`: Number of user searches for a specific journey on a given booking date\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Challenge Details\n",
    "\n",
    "You need to predict the **final demand (total seats booked)** for each route, **15 days in advance** of the travel date.\n",
    "\n",
    "#### âœ… Example\n",
    "\n",
    "- **Route**: Source City \"A\" â†’ Destination City \"B\"\n",
    "- **Date of Journey (DOJ)**: `30-Jan-2025`\n",
    "- **Prediction Date**: `16-Jan-2025` (15 days prior to DOJ)\n",
    "\n",
    "Your model should predict the **total expected bookings** on the prediction date for the journey date.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¥ Data Access\n",
    "\n",
    "Download the training and testing datasets from the links provided at the bottom of the problem statement.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Next Steps\n",
    "\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Feature Engineering\n",
    "3. Model Building\n",
    "4. Evaluation and Validation\n",
    "5. Final Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5b6b6-77bb-4971-b928-0e73ff9e6f68",
   "metadata": {},
   "source": [
    "## 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10103c3f-c6ca-4b96-a22d-a03eff62561b",
   "metadata": {},
   "source": [
    "Data - https://www.analyticsvidhya.com/datahack/contest/redbus-data-decode-hackathon-2025/\n",
    "1. train_csv => rows - 67200, columns - 4\n",
    "  - ['doj', 'srcid', 'destid', 'final_seatcount']\n",
    "2. test_csv  => rows - 5900, columns - 4\n",
    "  - ['route_key', 'doj', 'srcid', 'destid']\n",
    "3. transactions.csv => rows - 22661000, columns - 11\n",
    "  - ['doj', 'doi', 'srcid', 'destid', 'srcid_region', 'destid_region',\n",
    "       'srcid_tier', 'destid_tier', 'cumsum_seatcount', 'cumsum_searchcount',\n",
    "       'dbd']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098eb9b1-0d91-43d3-a301-c110976325df",
   "metadata": {},
   "source": [
    "### 2.1. Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462b4396-c9fc-4efb-97cb-a564c779f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec1008-a933-413a-9faf-776fc9b825a5",
   "metadata": {},
   "source": [
    "#### Import Data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8418bd66-9728-458b-89d6-a6f727846f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "transactions_data = pd.read_csv(\"transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20ab88e-cbea-4b71-889e-73fecff9c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data.copy()\n",
    "test_df = test_data.copy()\n",
    "test_df = test_df.drop('route_key', axis = 1)\n",
    "trans_df = transactions_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51320d7-efc0-48cb-ad25-13ffa5fa5bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape -> (67200, 4)\n",
      "transactions shape -> (2266100, 11)\n",
      "test shape -> (5900, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape ->\", train_df.shape)\n",
    "print(\"transactions shape ->\", trans_df.shape)\n",
    "print(\"test shape ->\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9c531-b9b2-4631-ada2-b98814a5349f",
   "metadata": {},
   "source": [
    "### 2.2 Merge data from transactions file to test and train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3838f6e4-6b33-424f-bc4a-233fb2c1bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_15 = trans_df[trans_df['dbd'] == 15]\n",
    "\n",
    "features = transaction_15[['doj', 'srcid', 'destid', 'srcid_region', 'destid_region',\n",
    "       'srcid_tier', 'destid_tier', 'cumsum_seatcount', 'cumsum_searchcount']]\n",
    "\n",
    "train_df = train_df.merge(features, on = ['doj', 'srcid', 'destid'], how = 'left')\n",
    "test_df = test_df.merge(features, on = ['doj', 'srcid', 'destid'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a091974-c914-4e9a-bdbd-d6c8e486c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [train_df, test_df]\n",
    "\n",
    "for df in dfs:\n",
    "    \n",
    "    df['doj'] = pd.to_datetime(df['doj'], format = '%Y-%m-%d')\n",
    "\n",
    "    # Extracting new columns with date columns\n",
    "    df['doj' + '_year'] = df['doj'].dt.year\n",
    "    df['doj' + '_month'] = df['doj'].dt.month\n",
    "    df['doj' + '_day'] = df['doj'].dt.day\n",
    "    df['doj' + '_dayofweek'] = df['doj'].dt.dayofweek\n",
    "    df['doj' + '_isweekend'] = df['doj'].dt.dayofweek.isin([5,6]).astype(int)\n",
    "\n",
    "    # Deleting the datetime datatype columns\n",
    "    df.drop(['doj'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd9032-5e1e-4056-9689-9c857e068bdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### info() of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bd7fc0-b1ce-4885-bf6f-63d8e5439d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67200 entries, 0 to 67199\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   srcid               67200 non-null  int64  \n",
      " 1   destid              67200 non-null  int64  \n",
      " 2   final_seatcount     67200 non-null  float64\n",
      " 3   srcid_region        67200 non-null  object \n",
      " 4   destid_region       67200 non-null  object \n",
      " 5   srcid_tier          67200 non-null  object \n",
      " 6   destid_tier         67200 non-null  object \n",
      " 7   cumsum_seatcount    67200 non-null  float64\n",
      " 8   cumsum_searchcount  67200 non-null  float64\n",
      " 9   doj_year            67200 non-null  int32  \n",
      " 10  doj_month           67200 non-null  int32  \n",
      " 11  doj_day             67200 non-null  int32  \n",
      " 12  doj_dayofweek       67200 non-null  int32  \n",
      " 13  doj_isweekend       67200 non-null  int64  \n",
      "dtypes: float64(3), int32(4), int64(3), object(4)\n",
      "memory usage: 6.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5900 entries, 0 to 5899\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   srcid               5900 non-null   int64  \n",
      " 1   destid              5900 non-null   int64  \n",
      " 2   srcid_region        5900 non-null   object \n",
      " 3   destid_region       5900 non-null   object \n",
      " 4   srcid_tier          5900 non-null   object \n",
      " 5   destid_tier         5900 non-null   object \n",
      " 6   cumsum_seatcount    5900 non-null   float64\n",
      " 7   cumsum_searchcount  5900 non-null   float64\n",
      " 8   doj_year            5900 non-null   int32  \n",
      " 9   doj_month           5900 non-null   int32  \n",
      " 10  doj_day             5900 non-null   int32  \n",
      " 11  doj_dayofweek       5900 non-null   int32  \n",
      " 12  doj_isweekend       5900 non-null   int64  \n",
      "dtypes: float64(2), int32(4), int64(3), object(4)\n",
      "memory usage: 507.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in dfs:\n",
    "    print(i.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc790d5b-06ff-4865-a0b6-ab2d1a2867d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Check for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0d9339-af55-48ed-93e7-900481a8af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcid                 0\n",
      "destid                0\n",
      "final_seatcount       0\n",
      "srcid_region          0\n",
      "destid_region         0\n",
      "srcid_tier            0\n",
      "destid_tier           0\n",
      "cumsum_seatcount      0\n",
      "cumsum_searchcount    0\n",
      "doj_year              0\n",
      "doj_month             0\n",
      "doj_day               0\n",
      "doj_dayofweek         0\n",
      "doj_isweekend         0\n",
      "dtype: int64 \n",
      "\n",
      "srcid                 0\n",
      "destid                0\n",
      "srcid_region          0\n",
      "destid_region         0\n",
      "srcid_tier            0\n",
      "destid_tier           0\n",
      "cumsum_seatcount      0\n",
      "cumsum_searchcount    0\n",
      "doj_year              0\n",
      "doj_month             0\n",
      "doj_day               0\n",
      "doj_dayofweek         0\n",
      "doj_isweekend         0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in dfs:   \n",
    "    print(i.isna().sum(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38661a6-3c6b-4fb9-8ac7-b4b7c94d5014",
   "metadata": {},
   "source": [
    "###### Insight\n",
    "- There are no Nulll values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec01b37-7313-4bc5-8203-4aa1c6491cb2",
   "metadata": {},
   "source": [
    "##### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb14ff78-b970-47d7-9d14-7fe99367e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in dfs:\n",
    "    print(i.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68eab39-5d0e-4cc8-b0c3-5c12e508f30b",
   "metadata": {},
   "source": [
    "###### Insights \n",
    "- No Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1bc52-b92a-489c-a2d7-734bf1fda217",
   "metadata": {},
   "source": [
    "#### Coverting objects into Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a591c62-a258-4146-9d78-1ee34d02e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_item_list = ['doj', 'doi']\n",
    "\n",
    "for df in dfs:\n",
    "    for col in date_item_list:\n",
    "        if col in df.columns and df[col].dtype != 'datetime64[ns]':\n",
    "            df[col] = pd.to_datetime(df[col], format = '%Y-%m-%d')\n",
    "\n",
    "            # Extracting new columns with date columns\n",
    "            df[col + '_year'] = df[col].dt.year\n",
    "            df[col + '_month'] = df[col].dt.month\n",
    "            df[col + '_day'] = df[col].dt.day\n",
    "            df[col + '_dayofweek'] = df[col].dt.dayofweek\n",
    "            df[col + '_isweekend'] = df[col].dt.dayofweek.isin([5,6]).astype(int)\n",
    "\n",
    "            # Deleting the datetime datatype columns\n",
    "            df.drop([col], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f3f04-a1e2-432f-84d5-e32a020e2693",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e3131a6-a9d0-4dee-bef6-1d580e41a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in train_df.columns if train_df[col].dtype == 'O']\n",
    "# ['srcid_region', 'destid_region', 'srcid_tier', 'destid_tier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b6f8d7-3e35-4e20-adcf-3ce57bbe4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(dtype = int)\n",
    "\n",
    "train_df[categorical_columns] = encoder.fit_transform(train_df[categorical_columns])\n",
    "test_df[categorical_columns] = encoder.fit_transform(test_df[categorical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98f93d-e637-4d35-8b69-125cd1cc2514",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "618a443d-c715-4bb8-b384-2ca13fed4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['final_seatcount'], axis = 1)\n",
    "y = train_df['final_seatcount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "175666cf-94bd-41f1-81b8-a0397d3a7bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['srcid', 'destid', 'final_seatcount', 'srcid_region', 'destid_region',\n",
      "       'srcid_tier', 'destid_tier', 'cumsum_seatcount', 'cumsum_searchcount',\n",
      "       'doj_year', 'doj_month', 'doj_day', 'doj_dayofweek', 'doj_isweekend'],\n",
      "      dtype='object')\n",
      "Index(['srcid', 'destid', 'srcid_region', 'destid_region', 'srcid_tier',\n",
      "       'destid_tier', 'cumsum_seatcount', 'cumsum_searchcount', 'doj_year',\n",
      "       'doj_month', 'doj_day', 'doj_dayofweek', 'doj_isweekend'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82239593-f0cb-4133-a5a9-201e6e5d615d",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87d4b1e4-c6cd-4ad0-a38d-05f4366ffcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(392.09103789169905)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred)) # 504.88480654785326\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
